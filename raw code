"""  Цель проекта - сделать удобный для франчайзеров инструмент, с помощью которго
они могут проанализировать рынок (как сторону спроса, так и сторону предложения).

Для достижения этой цели нами был написан код, состоящий из различных частей.
 Перейдем к описанию этих частей:
Анализ стороны предложения(анализ игроков на рынке)
Первые блоки кода парсят(статически+динамически) Авито и из него достают нужные данные, при этом сохраняют их в датафрейм. Данные, которые сохраняются,:


1. Название и ссылка
2. Цена
3. Надежность
4. Рейтинг
5. Зона выезда

Аналогично для яндекс услуг."""

import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote
import time
import pandas as pd
import random

def parse_avito(city):
"""
    Парсит объявления об уборке на Avito в указанном городе
    с первых 3 страниц и задержкой 6 секунд между запросами.
    Использует User-Agent и заголовки для обхода блокировки.
"""

    base_url = f'https://www.avito.ru/{city}/predlozheniya_uslug/uborka_klining/generalnaya_uborka/uborka_pomeshchenij'
    results = []
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    ]

    with requests.Session() as session:
        total_processed = 0
        for page in range(1, 4):
            try:
                url = f"{base_url}?p={page}" if page > 1 else base_url
                user_agent = random.choice(user_agents)
                headers = {
                    'User-Agent': user_agent,
                    'Referer': base_url,
                    'Accept-Language': 'ru',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                }

                response = session.get(url, headers=headers)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')
                items_container = soup.find('div', class_='items-items-pZX46')

                if not items_container:
                    print(f"Страница {page} - контейнер не найден")
                    continue

                items = items_container.find_all('div', class_="iva-item-body-GQomw")

                for item in items:
                    try:
                        title_tag = item.find('h3', itemprop="name")
                        title = title_tag.text.strip() if title_tag else "Без названия"

                        price_element = item.find('p', {'data-marker': 'item-price'}).find('strong')
                        price_text = price_element.text.strip() if price_element else "Цена не указана"

                        price_match = re.search(r'(\d+(?:\s*\d+)*)', price_text)
                        if price_match:
                          price = price_match.group(0).strip()
                        else:
                         price = "Цена не найдена"

                        link = urljoin(url, item.find('a')['href']) if item.find('a') else None
                        link = unquote(link) if link else None

                        results.append({
                            'title': title,
                            'price': price,
                            'link': link,
                            'page': page
                        })
                    except Exception as e:
                        print(f"Ошибка парсинга элемента: {e}")

                        total_processed += 1
                        print(f"Обработано объявлений: {total_processed}")
                    except Exception as e:
                        print(f"Ошибка парсинга элемента: {e}")

                if page < 3:
                    time.sleep(6)  

            except requests.exceptions.RequestException as e:
                print(f"Ошибка запроса страницы {page}: {e}")
                continue

    return results

if __name__ == '__main__':
    city = input("Введите город (латиницей, например 'moskva'): ").strip().lower()
    data = parse_avito(city)

    if data:
        print(f"\nНайдено объявлений: {len(data)}")
        df = pd.DataFrame(data)
        df.to_csv('avito_data.csv', index=False)
        print("Данные сохранены в avito_data.csv")
        df.to_excel('avito_data.xlsx', index=False)
        print("Данные сохранены в avito_data.xlsx")
    else:
        print("Объявления не найдены")

import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium import webdriver
from time import sleep

chrome_options = Options()
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
chrome_options.add_argument(f"user-agent={user_agent}")
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_argument("--headless")

service = Service(executable_path="C:/Users/krist/OneDrive/Desktop/проект прога/chrome-win64/chromedriver.exe")
driver = webdriver.Chrome(service=service, options=chrome_options)

try:
    data = pd.read_csv('avito_data.csv')
except FileNotFoundError:
    print("Ошибка: Файл avito_data.csv не найден.")
    driver.quit()
    exit()

def parse_page(url):
    result = {
        "rating": "Не найдено",
        "reviews_count": "Не найдено",
        "verified": "Не найдено",
        "documents_checked": "Не найдено",
        "trusted_seller": "Не найдено"
    }

    try:
        driver.get(url)
        sleep(2)

        try:
            rating_element = driver.find_element(By.XPATH, "//div[contains(@class, 'seller-info')]//meta[@itemprop='ratingValue']")
            result["rating"] = rating_element.get_attribute("content")
        except:
            pass

        try:
            reviews_count_element = driver.find_element(By.XPATH, "//a[contains(@class, 'seller-info__reviews')]//span[@itemprop='reviewCount']")
            result["reviews_count"] = reviews_count_element.text.strip()
        except:
            pass

        try:
            badges = driver.find_elements(By.XPATH, "//div[contains(@class, 'seller-info__badges')]//span")
            result["verified"] = "Да" if any("Проверено" in b.text for b in badges) else "Нет"
            result["documents_checked"] = "Да" if any("Документы проверены" in b.text for b in badges) else "Нет"
            result["trusted_seller"] = "Да" if any("Надежный продавец" in b.text for b in badges) else "Нет"
        except:
            pass

    except Exception as e:
        print(f"Ошибка при обработке {url}: {str(e)}")

    return result

parsed_data = []
for index, row in data.iterrows():
    print(f"Обрабатываю объявление {index + 1}/{len(data)}")
    parsed_info = parse_page(row['link'])

    parsed_data.append({
        "title": row['title'],
        "price": row['price'],
        "link": row['link'],
        **parsed_info
    })
    sleep(4)

result_df = pd.DataFrame(parsed_data)
result_df.to_csv('avito_parsed_results.csv', index=False)

driver.quit()
print("\nРезультаты сохранены в файл avito_parsed_results.csv!")
